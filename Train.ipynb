{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import data_helper\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from text_cnn import TextCNN\n",
    "from tensorflow.contrib import learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train_cnn - driver function for this respository. \n",
    "#### train_cnn takes two agruments\n",
    "    ### 1. entire data (features and labels)\n",
    "    ### 2. parameter values for CNN\n",
    "#### It first calls data_helper class which does the data cleaning and preprocessing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cnn(train_file, parameter_file):\n",
    "    \"\"\"Step 0: load sentences, labels, and training parameters\"\"\"\n",
    "    x_raw, y_raw, df, labels = data_helper.load_data_and_labels(train_file)\n",
    "\n",
    "    #for keys,values in label_dict.items():\n",
    "    #    print(keys)\n",
    "    #    print(values)\n",
    "        \n",
    "    params = json.loads(open(parameter_file).read())\n",
    "    \n",
    "    \"\"\"Step 1: pad each sentence to the same length and map each word to an id\"\"\"\n",
    "    max_document_length = max([len(x.split(' ')) for x in x_raw])\n",
    "    print('The maximum length of all sentences: {}'.format(max_document_length))\n",
    "    print('\\n')\n",
    "    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "    x = np.array(list(vocab_processor.fit_transform(x_raw)))\n",
    "    y = np.array(y_raw)\n",
    "    \n",
    "    \"\"\"Step 2: split the original dataset into train and test sets\"\"\"\n",
    "    x_, x_test, y_, y_test = train_test_split(x, y, test_size=0.1, random_state=42)\n",
    "\n",
    "    \"\"\"Step 3: shuffle the train set and split the train set into train and validation sets\"\"\"\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(y_)))\n",
    "    x_shuffled = x_[shuffle_indices]\n",
    "    y_shuffled = y_[shuffle_indices]\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_shuffled, y_shuffled, test_size=0.1)\n",
    "\n",
    "    \"\"\"Step 4: save the labels into labels.json\"\"\"\n",
    "    with open('./labels.json', 'w') as outfile:\n",
    "        json.dump(labels, outfile, indent=4)\n",
    "    print('x_train: {}, x_val: {}, x_test: {}'.format(len(x_train), len(x_val), len(x_test)))\n",
    "    print('y_train: {}, y_val: {}, y_test: {}'.format(len(y_train), len(y_val), len(y_test)))\n",
    "    print('\\n')\n",
    "    \"\"\"Step 5: build a graph and cnn object\"\"\"\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "        sess = tf.Session(config=session_conf)\n",
    "        with sess.as_default():\n",
    "            cnn = TextCNN(\n",
    "                sequence_length=x_train.shape[1],\n",
    "                num_classes=y_train.shape[1],\n",
    "                vocab_size=len(vocab_processor.vocabulary_),\n",
    "                embedding_size=params['embedding_dim'],\n",
    "                filter_sizes=list(map(int, params['filter_sizes'].split(\",\"))),\n",
    "                num_filters=params['num_filters'],\n",
    "                l2_reg_lambda=params['l2_reg_lambda'])\n",
    "\n",
    "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "            optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "            grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "            train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "            timestamp = str(int(time.time()))\n",
    "            out_dir = os.path.abspath(os.path.join(os.path.curdir, \"trained_model_\" + timestamp))\n",
    "\n",
    "            checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "            checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.makedirs(checkpoint_dir)\n",
    "            saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "            # One training step: train the model with one batch\n",
    "            def train_step(x_batch, y_batch):\n",
    "                feed_dict = {\n",
    "                    cnn.input_x: x_batch,\n",
    "                    cnn.input_y: y_batch,\n",
    "                    cnn.dropout_keep_prob: params['dropout_keep_prob']}\n",
    "                _, step, loss, acc = sess.run([train_op, global_step, cnn.loss, cnn.accuracy], feed_dict)\n",
    "\n",
    "            # One evaluation step: evaluate the model with one batch\n",
    "            def val_step(x_batch, y_batch):\n",
    "                feed_dict = {cnn.input_x: x_batch, cnn.input_y: y_batch, cnn.dropout_keep_prob: 1.0}\n",
    "                step, loss, acc, num_correct, predictions = sess.run([global_step, cnn.loss, cnn.accuracy, cnn.num_correct, cnn.predictions], feed_dict)\n",
    "                return num_correct, predictions\n",
    "\n",
    "            # Save the word_to_id map since predict.py needs it\n",
    "            vocab_processor.save(os.path.join(out_dir, \"vocab.pickle\"))\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            # Training starts here\n",
    "            train_batches = data_helper.batch_iter(list(zip(x_train, y_train)), params['batch_size'], params['num_epochs'])\n",
    "            best_accuracy, best_at_step = 0, 0\n",
    "\n",
    "            \"\"\"Step 6: train the cnn model with x_train and y_train (batch by batch)\"\"\"\n",
    "            for train_batch in train_batches:\n",
    "                x_train_batch, y_train_batch = zip(*train_batch)\n",
    "                train_step(x_train_batch, y_train_batch)\n",
    "                current_step = tf.train.global_step(sess, global_step)\n",
    "\n",
    "                \"\"\"Step 6.1: evaluate the model with x_val and y_val (batch by batch)\"\"\"\n",
    "                if current_step % params['evaluate_every'] == 0:\n",
    "                    val_batches = data_helper.batch_iter(list(zip(x_val, y_val)), params['batch_size'], 1)\n",
    "                    total_val_correct = 0\n",
    "                    for val_batch in val_batches:\n",
    "                        x_val_batch, y_val_batch = zip(*val_batch)\n",
    "                        num_val_correct, preds = val_step(x_val_batch, y_val_batch)\n",
    "                        total_val_correct += num_val_correct\n",
    "\n",
    "                    val_accuracy = float(total_val_correct) / len(y_val)\n",
    "                    print('Accuracy on validation set: {}'.format(val_accuracy))\n",
    "                    #print('\\n')\n",
    "                    \"\"\"Step 6.2: save the model if it is the best based on accuracy on val set\"\"\"\n",
    "                    if val_accuracy >= best_accuracy:\n",
    "                        best_accuracy, best_at_step = val_accuracy, current_step\n",
    "                        path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                        #print('Saved model at {} at step {}'.format(path, best_at_step))\n",
    "                        print('Best accuracy is {} at step {}'.format(best_accuracy, best_at_step))\n",
    "                        print('\\n')\n",
    "            print('The training is complete')\n",
    "            \"\"\"Step 7: predict x_test (batch by batch)\"\"\"\n",
    "            test_batches = data_helper.batch_iter(list(zip(x_test, y_test)), params['batch_size'], 1)\n",
    "            total_test_correct = 0\n",
    "            test_pred = []\n",
    "            test_labels = []\n",
    "            test_data = []\n",
    "            for test_batch in test_batches:\n",
    "                x_test_batch, y_test_batch = zip(*test_batch)\n",
    "                num_test_correct, preds = val_step(x_test_batch, y_test_batch)\n",
    "                total_test_correct += num_test_correct\n",
    "                test_pred.append(preds)\n",
    "                test_labels.append(y_test_batch)\n",
    "                test_data.append(x_test_batch)\n",
    "\n",
    "            test_accuracy = float(total_test_correct) / len(y_test)\n",
    "            print('\\n')\n",
    "            print('Accuracy on test set is {} based on the best model {}'.format(test_accuracy, path))\n",
    "            print('\\n')\n",
    "            return test_pred, test_labels, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum length of all sentences: 912\n",
      "\n",
      "\n",
      "x_train: 54112, x_val: 6013, x_test: 6681\n",
      "y_train: 54112, y_val: 6013, y_test: 6681\n",
      "\n",
      "\n",
      "Accuracy on validation set: 0.4418759354731415\n",
      "Best accuracy is 0.4418759354731415 at step 200\n",
      "\n",
      "\n",
      "Accuracy on validation set: 0.5727590221187427\n",
      "Best accuracy is 0.5727590221187427 at step 400\n",
      "\n",
      "\n",
      "Accuracy on validation set: 0.6532512888741061\n",
      "Best accuracy is 0.6532512888741061 at step 600\n",
      "\n",
      "\n",
      "Accuracy on validation set: 0.6662231831032762\n",
      "Best accuracy is 0.6662231831032762 at step 800\n",
      "\n",
      "\n",
      "Accuracy on validation set: 0.7019790454016298\n",
      "Best accuracy is 0.7019790454016298 at step 1000\n",
      "\n",
      "\n",
      "Accuracy on validation set: 0.7144520206219857\n",
      "Best accuracy is 0.7144520206219857 at step 1200\n",
      "\n",
      "\n",
      "The training is complete\n",
      "\n",
      "\n",
      "Accuracy on test set is 0.7374644514294267 based on the best model /Users/zairah@ibm.com/Documents/IBM/ImagingTeam/SampleWork/Data/Flipkart/CNN/trained_model_1513772897/checkpoints/model-1200\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p, y, x_test = train_cnn('data/consumer_complaints.csv.zip', 'parameters.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Bank account or Service', 'Consumer Loan', 'Credit card', 'Credit reporting', \\\n",
    "          'Debt collection', 'Money transfers', 'Mortgage', \\\n",
    "          'Other financial service', 'Payday loan', 'Prepaid card', 'Student loan']\n",
    "one_hot = np.zeros((len(labels), len(labels)), int)\n",
    "np.fill_diagonal(one_hot, 1)\n",
    "label_dict = dict(zip(labels, one_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] \n",
    "values = ['Bank account or Service', 'Consumer Loan', 'Credit card', 'Credit reporting', \\\n",
    "          'Debt collection', 'Money transfers', 'Mortgage', \\\n",
    "          'Other financial service', 'Payday loan', 'Prepaid card', 'Student loan']\n",
    "label_int_dict = dict(zip(labels, values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoriesy = []\n",
    "for i in y:\n",
    "    #get actual labels for each epoch\n",
    "    for x in i:\n",
    "        categoriesy.append(x)\n",
    "#len(categoriesy)\n",
    "caty = []\n",
    "for line in categoriesy:        \n",
    "    for key, val in label_dict.items():\n",
    "        if np.array_equal(line, val):\n",
    "            caty.append(key)\n",
    "#len(caty)\n",
    "#caty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df = pd.DataFrame(caty, columns=['y'])\n",
    "#y_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_funcp(a):\n",
    "    for key, val in label_int_dict.items():\n",
    "        if a == key:\n",
    "            return val\n",
    "categoriesp = []\n",
    "for i in p:\n",
    "    #get actual labels for each epoch\n",
    "    for x in i:\n",
    "        categoriesp.append(x)\n",
    "    \n",
    "#len(categoriesp)\n",
    "catp = []\n",
    "for line in categoriesp:        \n",
    "    catp.append(label_int_dict.get(line))\n",
    "#len(catp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_df = pd.DataFrame(catp, columns=['p'])\n",
    "#p_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual_label</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6671</th>\n",
       "      <td>Credit reporting</td>\n",
       "      <td>Credit reporting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6672</th>\n",
       "      <td>Debt collection</td>\n",
       "      <td>Debt collection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6673</th>\n",
       "      <td>Debt collection</td>\n",
       "      <td>Debt collection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6674</th>\n",
       "      <td>Debt collection</td>\n",
       "      <td>Mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6675</th>\n",
       "      <td>Credit reporting</td>\n",
       "      <td>Credit reporting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6676</th>\n",
       "      <td>Credit reporting</td>\n",
       "      <td>Credit reporting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6677</th>\n",
       "      <td>Mortgage</td>\n",
       "      <td>Mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6678</th>\n",
       "      <td>Debt collection</td>\n",
       "      <td>Debt collection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6679</th>\n",
       "      <td>Consumer Loan</td>\n",
       "      <td>Debt collection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6680</th>\n",
       "      <td>Credit reporting</td>\n",
       "      <td>Credit reporting</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          actual_label   predicted_label\n",
       "6671  Credit reporting  Credit reporting\n",
       "6672   Debt collection   Debt collection\n",
       "6673   Debt collection   Debt collection\n",
       "6674   Debt collection          Mortgage\n",
       "6675  Credit reporting  Credit reporting\n",
       "6676  Credit reporting  Credit reporting\n",
       "6677          Mortgage          Mortgage\n",
       "6678   Debt collection   Debt collection\n",
       "6679     Consumer Loan   Debt collection\n",
       "6680  Credit reporting  Credit reporting"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df = pd.DataFrame()\n",
    "final_df['actual_label'] = y_df['y']\n",
    "final_df['predicted_label'] = p_df['p']\n",
    "final_df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
